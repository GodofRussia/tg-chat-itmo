#!/usr/bin/env python3
"""
–§–∏–Ω–∞–ª—å–Ω—ã–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º –º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä—ã
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª—é–±—ã–µ URL-—ã –∏ –ø–∞—Ä—Å–∏–Ω–≥ PDF —Å —É—á–µ–±–Ω—ã–º–∏ –ø–ª–∞–Ω–∞–º–∏
–°–∫–∞—á–∏–≤–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç PDF —Ñ–∞–π–ª—ã –≤ –ø–∞–º—è—Ç–∏ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞ –¥–∏—Å–∫
"""

import re
import json
import sys
import io
import tempfile
from typing import Dict, List, Any, Optional

def extract_curriculum_data(text: str) -> Dict[str, Any]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ö –∏–∑ —Ç–µ–∫—Å—Ç–∞ —É—á–µ–±–Ω–æ–≥–æ –ø–ª–∞–Ω–∞
    """
    curriculum = {
        "program_name": "",
        "semesters": {},
        "elective_courses": [],
        "mandatory_courses": [],
        "all_courses": []
    }

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã
    program_match = re.search(r'–û–ü\s+(.+?)(?:–°–µ–º–µ—Å—Ç—Ä—ã|$)', text)
    if program_match:
        curriculum["program_name"] = program_match.group(1).strip()

    # –ü–∞—Ä—Å–∏–º –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã –ø–æ —Å–µ–º–µ—Å—Ç—Ä–∞–º
    lines = text.split('\n')
    current_semester = None
    current_section = None

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ–º–µ—Å—Ç—Ä
        semester_match = re.search(r'(\d+)\s*—Å–µ–º–µ—Å—Ç—Ä', line)
        if semester_match:
            current_semester = int(semester_match.group(1))
            if current_semester not in curriculum["semesters"]:
                curriculum["semesters"][current_semester] = {
                    "mandatory": [],
                    "elective": []
                }
            continue

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–∏—Å—Ü–∏–ø–ª–∏–Ω
        if "–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã" in line:
            current_section = "mandatory"
            continue
        elif "–ü—É–ª –≤—ã–±–æ—Ä–Ω—ã—Ö –¥–∏—Å—Ü–∏–ø–ª–∏–Ω" in line or "–≤—ã–±–æ—Ä–Ω—ã—Ö" in line.lower():
            current_section = "elective"
            continue

        # –ü–∞—Ä—Å–∏–º –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—É - –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ
        # –§–æ—Ä–º–∞—Ç: –Ω–æ–º–µ—Ä_—Å–µ–º–µ—Å—Ç—Ä–∞ + –Ω–∞–∑–≤–∞–Ω–∏–µ + —á–∏—Å–ª–æ (–∫—Ä–µ–¥–∏—Ç—ã+—á–∞—Å—ã —Å–ª–∏—Ç–Ω–æ)
        course_match = re.match(r'^(\d+)(.+?)\s+(\d+)\s*$', line)
        if course_match:
            semester_num = int(course_match.group(1))
            course_name = course_match.group(2).strip()
            combined_number = course_match.group(3)

            # –†–∞–∑–¥–µ–ª—è–µ–º –∫—Ä–µ–¥–∏—Ç—ã –∏ —á–∞—Å—ã
            # –û–±—ã—á–Ω–æ –ø–µ—Ä–≤–∞—è —Ü–∏—Ñ—Ä–∞ - –∫—Ä–µ–¥–∏—Ç—ã, –æ—Å—Ç–∞–ª—å–Ω—ã–µ - —á–∞—Å—ã
            if len(combined_number) >= 4:
                credits = int(combined_number[0])  # –ü–µ—Ä–≤–∞—è —Ü–∏—Ñ—Ä–∞
                hours = int(combined_number[1:])   # –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ü–∏—Ñ—Ä—ã
            else:
                credits = int(combined_number)
                hours = 0

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã
            if current_section is None:
                # –ï—Å–ª–∏ —Å–µ–∫—Ü–∏—è –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞, –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
                if any(keyword in course_name.lower() for keyword in ['–æ–±—è–∑–∞—Ç–µ–ª—å–Ω', '–≤–æ—Ä–∫—à–æ–ø']):
                    section_type = "mandatory"
                else:
                    section_type = "elective"
            else:
                section_type = current_section

            course_info = {
                "name": course_name,
                "credits": credits,
                "hours": hours,
                "semester": semester_num,
                "type": section_type
            }

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ–º–µ—Å—Ç—Ä –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if semester_num not in curriculum["semesters"]:
                curriculum["semesters"][semester_num] = {
                    "mandatory": [],
                    "elective": []
                }

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å–ø–∏—Å–∫–∏
            if section_type == "mandatory":
                curriculum["semesters"][semester_num]["mandatory"].append(course_info)
                curriculum["mandatory_courses"].append(course_info)
            else:
                curriculum["semesters"][semester_num]["elective"].append(course_info)
                curriculum["elective_courses"].append(course_info)

            curriculum["all_courses"].append(course_info)

    return curriculum

def categorize_subjects(curriculum: Dict) -> Dict[str, List[Dict]]:
    """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–º–µ—Ç—ã –ø–æ –æ–±–ª–∞—Å—Ç—è–º"""
    categories = {
        'machine_learning': {
            'keywords': ['–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', 'machine learning', 'ml', '–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ',
                       'deep learning', '–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏', 'neural networks', '–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ'],
            'description': '–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –ò–ò'
        },
        'programming': {
            'keywords': ['–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ', 'python', 'c++', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '–≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π',
                       '–º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤', 'backend', '—è–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è'],
            'description': '–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞'
        },
        'computer_vision': {
            'keywords': ['–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ', 'computer vision', '–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', '–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π',
                       '–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', '–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ'],
            'description': '–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ'
        },
        'nlp': {
            'keywords': ['–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞', 'nlp', '–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤', '—è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏',
                       'llm', '–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏', '—Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–≥–æ'],
            'description': '–û–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞'
        },
        'data_science': {
            'keywords': ['–¥–∞–Ω–Ω—ã–µ', 'data', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', '–∞–Ω–∞–ª–∏–∑', '–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è',
                       '–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã', 'big data', '–±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö'],
            'description': '–ù–∞—É–∫–∞ –æ –¥–∞–Ω–Ω—ã—Ö'
        },
        'systems': {
            'keywords': ['—Å–∏—Å—Ç–µ–º—ã', 'mlops', 'devops', '–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü–∏—è', 'gpu', 'unix',
                       '–±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö', '–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞', '–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞'],
            'description': '–°–∏—Å—Ç–µ–º—ã –∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞'
        },
        'product': {
            'keywords': ['–ø—Ä–æ–¥—É–∫—Ç', 'product', '–±–∏–∑–Ω–µ—Å', '—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', '–ø—Ä–æ–µ–∫—Ç',
                       '–∞–Ω–∞–ª–∏—Ç–∏–∫–∞', '–¥–∏–∑–∞–π–Ω', '–ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏–µ'],
            'description': '–ü—Ä–æ–¥—É–∫—Ç–æ–≤–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ'
        }
    }

    categorized = {category: [] for category in categories.keys()}
    categorized['other'] = []

    all_courses = curriculum.get('all_courses', [])

    for course in all_courses:
        course_name = course['name'].lower()
        assigned = False

        for category, category_data in categories.items():
            if any(keyword in course_name for keyword in category_data['keywords']):
                categorized[category].append(course)
                assigned = True
                break

        if not assigned:
            categorized['other'].append(course)

    return categorized

def generate_recommendations(background_text: str, curriculum: Dict) -> Dict:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Ñ–∏–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ–º"""

    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
    user_profiles = {
        'fullstack_developer': {
            'keywords': [
                # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
                '—Ñ—É–ª–ª—Å—Ç–µ–∫', 'fullstack', 'full-stack', 'full stack',
                '—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫', '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç', 'developer', 'programmer',
                '—Ñ—Ä–æ–Ω—Ç–µ–Ω–¥', 'frontend', 'front-end', '–±—ç–∫–µ–Ω–¥', 'backend', 'back-end',
                '—Å–µ—Ä–≤–∏—Å—ã', 'services', '–≤–µ–±', 'web', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è', 'applications',
                '–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞', 'infrastructure', 'devops',
                # –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
                'python', 'javascript', 'react', 'node', 'django', 'flask',
                'api', 'rest', '–º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å—ã', 'microservices',
                # –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã
                '—è–Ω–¥–µ–∫—Å', 'yandex', '–æ–ø—ã—Ç', 'experience', '—Ä–∞–±–æ—Ç–∞—é', '—Ä–∞–±–æ—Ç–∞–ª'
            ],
            'preferences': ['programming', 'systems', 'machine_learning'],
            'description': 'Fullstack —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫'
        },
        'ml_engineer': {
            'keywords': [
                'ml engineer', '–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', 'machine learning', 'ml',
                '–º–æ–¥–µ–ª–∏', 'models', '–∞–ª–≥–æ—Ä–∏—Ç–º—ã', 'algorithms', '–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏',
                'deep learning', '–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ', 'tensorflow', 'pytorch',
                'sklearn', 'data science', 'ai', 'artificial intelligence',
                'kaggle', '—Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è', 'competitions'
            ],
            'preferences': ['machine_learning', 'programming', 'systems'],
            'description': 'ML Engineer'
        },
        'data_scientist': {
            'keywords': [
                'data scientist', '–¥–∞–Ω–Ω—ã–µ', 'data', '–∞–Ω–∞–ª–∏—Ç–∏–∫', 'analyst',
                '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', 'statistics', '–∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö', 'data analysis',
                'pandas', 'numpy', 'jupyter', 'visualization', '–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è',
                'bi', 'business intelligence', '–¥–∞—à–±–æ—Ä–¥—ã', 'dashboards'
            ],
            'preferences': ['data_science', 'machine_learning', 'programming'],
            'description': 'Data Scientist'
        },
        'cv_specialist': {
            'keywords': [
                'computer vision', '–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ', 'cv', '–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è',
                'images', 'opencv', '–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'image processing',
                '—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ', 'recognition', '–¥–µ—Ç–µ–∫—Ü–∏—è', 'detection',
                '—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è', 'segmentation', 'yolo', 'cnn'
            ],
            'preferences': ['computer_vision', 'machine_learning', 'programming'],
            'description': 'Computer Vision —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç'
        },
        'nlp_specialist': {
            'keywords': [
                'nlp', 'natural language processing', '–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —è–∑—ã–∫',
                '–æ–±—Ä–∞–±–æ—Ç–∫–∞ —è–∑—ã–∫–∞', '—Ç–µ–∫—Å—Ç', 'text', '—è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏',
                'language models', 'llm', 'bert', 'gpt', 'transformers',
                '—á–∞—Ç–±–æ—Ç—ã', 'chatbots', 'sentiment', '—Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å'
            ],
            'preferences': ['nlp', 'machine_learning', 'programming'],
            'description': 'NLP —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç'
        },
        'product_manager': {
            'keywords': [
                'product manager', '–ø—Ä–æ–¥—É–∫—Ç', 'product', '–º–µ–Ω–µ–¥–∂–µ—Ä', 'manager',
                '—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', 'management', '–±–∏–∑–Ω–µ—Å', 'business', '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è',
                'strategy', 'roadmap', '–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ', 'planning', '–ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–π',
                '–∞–Ω–∞–ª–∏—Ç–∏–∫–∞', 'analytics', '–º–µ—Ç—Ä–∏–∫–∏', 'metrics', 'a/b —Ç–µ—Å—Ç—ã'
            ],
            'preferences': ['product', 'data_science', 'machine_learning'],
            'description': 'Product Manager'
        },
        'systems_architect': {
            'keywords': [
                '–∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä', 'architect', '—Å–∏—Å—Ç–µ–º—ã', 'systems', '–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞',
                'architecture', '–≤—ã—Å–æ–∫–æ–Ω–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ', 'highload', 'high-load',
                '–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ', 'scaling', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å', 'performance',
                '–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞', 'infrastructure', '–æ–±–ª–∞–∫–æ', 'cloud',
                'kubernetes', 'docker', '–º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å—ã'
            ],
            'preferences': ['systems', 'programming', 'machine_learning'],
            'description': '–°–∏—Å—Ç–µ–º–Ω—ã–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä'
        },
        'backend_developer': {
            'keywords': [
                'backend', '–±—ç–∫–µ–Ω–¥', '—Å–µ—Ä–≤–µ—Ä–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', 'server-side',
                'api', 'rest', 'graphql', '–±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö', 'databases',
                'postgresql', 'mysql', 'mongodb', 'redis',
                'java', 'go', 'c#', 'node.js', 'spring'
            ],
            'preferences': ['programming', 'systems', 'data_science'],
            'description': 'Backend —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫'
        },
        'frontend_developer': {
            'keywords': [
                'frontend', '—Ñ—Ä–æ–Ω—Ç–µ–Ω–¥', '–∫–ª–∏–µ–Ω—Ç—Å–∫–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', 'client-side',
                'react', 'vue', 'angular', 'javascript', 'typescript',
                'html', 'css', 'ui', 'ux', '–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã', 'interfaces'
            ],
            'preferences': ['programming', 'product', 'machine_learning'],
            'description': 'Frontend —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫'
        },
        'mobile_developer': {
            'keywords': [
                'mobile', '–º–æ–±–∏–ª—å–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', 'android', 'ios',
                'react native', 'flutter', 'swift', 'kotlin',
                '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è', 'apps', '–º–æ–±–∏–ª—å–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è'
            ],
            'preferences': ['programming', 'product', 'systems'],
            'description': 'Mobile —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫'
        },
        'devops_engineer': {
            'keywords': [
                'devops', '–¥–µ–ø–ª–æ–π', 'deployment', 'ci/cd', 'jenkins',
                'gitlab', 'github actions', 'terraform', 'ansible',
                '–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥', 'monitoring', '–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ', 'logging'
            ],
            'preferences': ['systems', 'programming', 'machine_learning'],
            'description': 'DevOps Engineer'
        },
        'qa_engineer': {
            'keywords': [
                'qa', '—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ', 'testing', '–∞–≤—Ç–æ—Ç–µ—Å—Ç—ã', 'automation',
                'selenium', 'pytest', '–∫–∞—á–µ—Å—Ç–≤–æ', 'quality assurance',
                '–±–∞–≥–∏', 'bugs', '—Ç–µ—Å—Ç-–∫–µ–π—Å—ã', 'test cases'
            ],
            'preferences': ['programming', 'systems', 'product'],
            'description': 'QA Engineer'
        },
        'business_analyst': {
            'keywords': [
                'business analyst', '–±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫', '–∞–Ω–∞–ª–∏—Ç–∏–∫',
                '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', 'requirements', '–ø—Ä–æ—Ü–µ—Å—Å—ã', 'processes',
                '–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è', 'documentation', 'stakeholders'
            ],
            'preferences': ['product', 'data_science', 'programming'],
            'description': '–ë–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫'
        },
        'researcher': {
            'keywords': [
                '–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å', 'researcher', '–Ω–∞—É–∫–∞', 'science',
                '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è', 'research', '–ø—É–±–ª–∏–∫–∞—Ü–∏–∏', 'papers',
                '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã', 'experiments', 'phd', '–∫–∞–Ω–¥–∏–¥–∞—Ç –Ω–∞—É–∫'
            ],
            'preferences': ['machine_learning', 'data_science', 'programming'],
            'description': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å'
        },
        'student': {
            'keywords': [
                '—Å—Ç—É–¥–µ–Ω—Ç', 'student', '—É—á—É—Å—å', 'studying', '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç',
                '–≤—É–∑', '–∏–Ω—Å—Ç–∏—Ç—É—Ç', '–∫—É—Ä—Å–æ–≤—ã–µ', '–¥–∏–ø–ª–æ–º–Ω–∞—è', 'thesis',
                '–±–∞–∫–∞–ª–∞–≤—Ä', 'bachelor', '–º–∞–≥–∏—Å—Ç—Ä', 'master'
            ],
            'preferences': ['machine_learning', 'programming', 'data_science'],
            'description': '–°—Ç—É–¥–µ–Ω—Ç'
        }
    }

    # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    background_lower = background_text.lower()
    profile_scores = {}

    for profile_name, profile_data in user_profiles.items():
        score = 0
        matched_keywords = []

        for keyword in profile_data['keywords']:
            if keyword in background_lower:
                score += 1
                matched_keywords.append(keyword)

        # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        if score > 0:
            score = score * (1 + len(matched_keywords) * 0.1)

        profile_scores[profile_name] = {
            'score': score,
            'matched_keywords': matched_keywords
        }

    # –ù–∞—Ö–æ–¥–∏–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–π –ø—Ä–æ—Ñ–∏–ª—å
    best_profile = None
    best_score = 0

    for profile_name, data in profile_scores.items():
        if data['score'] > best_score:
            best_score = data['score']
            best_profile = profile_name

    if best_profile and best_score > 0:
        dominant_profile = best_profile
        preferences = user_profiles[dominant_profile]['preferences']
        profile_description = user_profiles[dominant_profile]['description']
    else:
        # –ï—Å–ª–∏ –ø—Ä–æ—Ñ–∏–ª—å –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –æ–±—â–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        if any(word in background_lower for word in ['–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç', '—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫', 'developer', '–∫–æ–¥', 'code']):
            dominant_profile = 'general_developer'
            preferences = ['programming', 'systems', 'machine_learning']
            profile_description = '–†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ (–æ–±—â–∏–π –ø—Ä–æ—Ñ–∏–ª—å)'
        else:
            dominant_profile = None
            preferences = ['machine_learning', 'programming', 'data_science']
            profile_description = '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω'

    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ–º –ø—Ä–µ–¥–º–µ—Ç—ã
    categorized = categorize_subjects(curriculum)

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π
    recommendations = []

    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    for i, category in enumerate(preferences):
        subjects = categorized.get(category, [])
        elective_subjects = [s for s in subjects if s.get('type') == 'elective']

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫—Ä–µ–¥–∏—Ç–∞–º (–±–æ–ª—å—à–µ –∫—Ä–µ–¥–∏—Ç–æ–≤ = –≤–∞–∂–Ω–µ–µ)
        elective_subjects.sort(key=lambda x: x.get('credits', 0), reverse=True)

        # –ë–µ—Ä–µ–º —Ç–æ–ø –ø—Ä–µ–¥–º–µ—Ç—ã –∏–∑ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        limit = max(1, 4 - i)  # –ü–µ—Ä–≤–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è - 4 –ø—Ä–µ–¥–º–µ—Ç–∞, –≤—Ç–æ—Ä–∞—è - 3, —Ç—Ä–µ—Ç—å—è - 2
        for subject in elective_subjects[:limit]:
            reason = f"–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è {profile_description}" if dominant_profile else "–ë–∞–∑–æ–≤–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è"
            if i == 0:
                reason += " (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–∞—è –æ–±–ª–∞—Å—Ç—å)"

            recommendations.append({
                **subject,
                'category': category,
                'reason': reason,
                'priority': len(preferences) - i  # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ—Ç 3 –¥–æ 1
            })

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∏ –∫—Ä–µ–¥–∏—Ç–∞–º
    recommendations.sort(key=lambda x: (x.get('priority', 0), x.get('credits', 0)), reverse=True)

    return {
        'user_profile': profile_description,
        'dominant_profile_key': dominant_profile,
        'profile_scores': {k: v['score'] for k, v in profile_scores.items()},
        'matched_keywords': profile_scores.get(best_profile, {}).get('matched_keywords', []) if best_profile else [],
        'recommendations': recommendations[:10],  # –¢–æ–ø-10
        'category_distribution': {cat: len(subjects) for cat, subjects in categorized.items()},
        'analysis': {
            'total_courses': len(curriculum.get('all_courses', [])),
            'elective_courses': len(curriculum.get('elective_courses', [])),
            'categories_found': len([cat for cat, subjects in categorized.items() if subjects])
        }
    }

def get_profile_examples() -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª–µ–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""

    examples = {
        'üë®‚Äçüíª –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏': [
            'Fullstack —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –∏–∑ –Ø–Ω–¥–µ–∫—Å–∞, –æ–ø—ã—Ç 3 –≥–æ–¥–∞, –∑–Ω–∞—é Python, React, —Ä–∞–±–æ—Ç–∞—é —Å –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞–º–∏',
            'Backend —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–∞ Java, —Ä–∞–±–æ—Ç–∞—é –≤ –±–∞–Ω–∫–µ, –æ–ø—ã—Ç —Å Spring Boot –∏ PostgreSQL',
            'Frontend —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫, –¥–µ–ª–∞—é –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã –Ω–∞ React –∏ Vue, –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Å—å UX',
            'Mobile —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ iOS, –ø–∏—à—É –Ω–∞ Swift, —Ö–æ—á—É –∏–∑—É—á–∏—Ç—å ML –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π'
        ],
        'ü§ñ ML/AI —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã': [
            'ML Engineer –≤ —Å—Ç–∞—Ä—Ç–∞–ø–µ, –∑–∞–Ω–∏–º–∞—é—Å—å –æ–±—É—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π, –∑–Ω–∞—é TensorFlow –∏ PyTorch',
            'Data Scientist, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é –¥–∞–Ω–Ω—ã–µ –≤ e-commerce, —Ä–∞–±–æ—Ç–∞—é —Å pandas –∏ sklearn',
            'Computer Vision —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç, –¥–µ–ª–∞—é —Å–∏—Å—Ç–µ–º—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è, –æ–ø—ã—Ç —Å OpenCV',
            'NLP –∏–Ω–∂–µ–Ω–µ—Ä, —Å–æ–∑–¥–∞—é —á–∞—Ç–±–æ—Ç—ã –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–µ–∫—Å—Ç—ã, —Ä–∞–±–æ—Ç–∞–ª —Å BERT –∏ GPT'
        ],
        'üìä –ê–Ω–∞–ª–∏—Ç–∏–∫–∏ –∏ –º–µ–Ω–µ–¥–∂–µ—Ä—ã': [
            'Product Manager –≤ IT, —É–ø—Ä–∞–≤–ª—è—é –ø—Ä–æ–¥—É–∫—Ç–æ–º, —Ö–æ—á—É –ø–æ–Ω–∏–º–∞—Ç—å AI –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π',
            '–ë–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫, —Ä–∞–±–æ—Ç–∞—é —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏, –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Å—å data science',
            '–ê–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö, —Å—Ç—Ä–æ—é –¥–∞—à–±–æ—Ä–¥—ã –≤ Tableau, —Ö–æ—á—É –∏–∑—É—á–∏—Ç—å –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ'
        ],
        'üîß –ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ —Å–∏—Å—Ç–µ–º—ã': [
            'DevOps –∏–Ω–∂–µ–Ω–µ—Ä, –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é CI/CD, —Ä–∞–±–æ—Ç–∞—é —Å Kubernetes –∏ Docker',
            '–°–∏—Å—Ç–µ–º–Ω—ã–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä, –ø—Ä–æ–µ–∫—Ç–∏—Ä—É—é –≤—ã—Å–æ–∫–æ–Ω–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã',
            'QA Engineer, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É—é —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, —Ö–æ—á—É –∏–∑—É—á–∏—Ç—å ML –¥–ª—è —Ç–µ—Å—Ç–æ–≤'
        ],
        'üéì –°—Ç—É–¥–µ–Ω—Ç—ã –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏': [
            '–°—Ç—É–¥–µ–Ω—Ç 4 –∫—É—Ä—Å–∞ –ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–µ, –ø–∏—à—É –¥–∏–ø–ª–æ–º–Ω—É—é –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é',
            '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å –≤ –æ–±–ª–∞—Å—Ç–∏ AI, –ø—É–±–ª–∏–∫—É—é—Å—å –≤ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è—Ö, –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã',
            '–ú–∞–≥–∏—Å—Ç—Ä–∞–Ω—Ç –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ, —Ö–æ—á—É –ø—Ä–∏–º–µ–Ω–∏—Ç—å –∑–Ω–∞–Ω–∏—è –≤ data science'
        ]
    }

    result = "üìù **–ü—Ä–∏–º–µ—Ä—ã –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª—è:**\n\n"

    for category, profiles in examples.items():
        result += f"**{category}:**\n"
        for profile in profiles:
            result += f"‚Ä¢ `{profile}`\n"
        result += "\n"

    result += "üí° **–ß—Ç–æ —É–∫–∞–∑—ã–≤–∞—Ç—å:**\n"
    result += "‚Ä¢ –¢–µ–∫—É—â—É—é –ø—Ä–æ—Ñ–µ—Å—Å–∏—é/–¥–æ–ª–∂–Ω–æ—Å—Ç—å\n"
    result += "‚Ä¢ –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã (–≤ –≥–æ–¥–∞—Ö)\n"
    result += "‚Ä¢ –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã\n"
    result += "‚Ä¢ –ö–æ–º–ø–∞–Ω–∏—é –∏–ª–∏ —Å—Ñ–µ—Ä—É –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n"
    result += "‚Ä¢ –ò–Ω—Ç–µ—Ä–µ—Å—ã –∏ —Ü–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è\n"
    result += "‚Ä¢ –ü—Ä–æ–µ–∫—Ç—ã –∏–ª–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è\n\n"

    result += "üéØ **–°–∏—Å—Ç–µ–º–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç 15+ –ø—Ä–æ—Ñ–∏–ª–µ–π:**\n"
    result += "Fullstack, Backend, Frontend, Mobile, ML Engineer, Data Scientist, "
    result += "Computer Vision, NLP, Product Manager, DevOps, QA, Business Analyst, "
    result += "Systems Architect, Researcher, Student"

    return result

def download_and_parse_pdf(pdf_url: str) -> Optional[Dict[str, Any]]:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç PDF —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç—å –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑ –Ω–µ–≥–æ –¥–∞–Ω–Ω—ã–µ –æ –∫—É—Ä—Å–∞—Ö
    –†–∞–±–æ—Ç–∞–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤ –ø–∞–º—è—Ç–∏, –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∞–π–ª—ã –Ω–∞ –¥–∏—Å–∫
    """
    try:
        import requests

        print(f"üì• –ó–∞–≥—Ä—É–∂–∞—é PDF –∏–∑ API: {pdf_url}")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/pdf,*/*'
        }

        response = requests.get(pdf_url, headers=headers, timeout=60)
        response.raise_for_status()

        print(f"‚úÖ PDF –∑–∞–≥—Ä—É–∂–µ–Ω: {len(response.content)} –±–∞–π—Ç")

        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç–∏ (–Ω–µ –Ω–∞ –¥–∏—Å–∫–µ!)
        pdf_content = io.BytesIO(response.content)

        try:
            import PyPDF2

            # –ß–∏—Ç–∞–µ–º PDF –∏–∑ –ø–∞–º—è—Ç–∏
            pdf_reader = PyPDF2.PdfReader(pdf_content)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            full_text = ""
            for page in pdf_reader.pages:
                full_text += page.extract_text() + "\n"

            print(f"üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(full_text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {len(pdf_reader.pages)} —Å—Ç—Ä–∞–Ω–∏—Ü")

            # –ü–∞—Ä—Å–∏–º —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω
            curriculum = extract_curriculum_data(full_text)

            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
            pdf_content.close()
            del pdf_content
            del response

            print(f"üéì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(curriculum['all_courses'])} –¥–∏—Å—Ü–∏–ø–ª–∏–Ω")

            return {
                "curriculum": curriculum,
                "pdf_text_length": len(full_text),
                "pdf_pages": len(pdf_reader.pages)
            }

        except ImportError:
            print("‚ùå –î–ª—è —Ä–∞–±–æ—Ç—ã —Å PDF —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install PyPDF2")
            return None
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ PDF: {e}")
            return None

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ PDF {pdf_url}: {e}")
        return None

def extract_js_urls_and_analyze(html_content: str, base_url: str) -> List[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ JavaScript —Ñ–∞–π–ª—ã —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Ö –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç API endpoints
    """
    from bs4 import BeautifulSoup
    from urllib.parse import urljoin
    import requests

    soup = BeautifulSoup(html_content, "html.parser")
    js_urls = []

    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ script —Ç–µ–≥–∏ —Å src
    for script in soup.find_all("script", src=True):
        src = script.get("src")
        if src:
            if src.startswith("http"):
                js_urls.append(src)
            else:
                js_urls.append(urljoin(base_url, src))

    return js_urls

def analyze_js_for_api_endpoints(js_content: str) -> List[str]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç JavaScript –∫–æ–¥ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç API endpoints
    """
    endpoints = []

    # –ò—â–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã API endpoints
    patterns = [
        r'["\']([^"\']*api[^"\']*)["\']',  # –û–±—â–∏–µ API –ø—É—Ç–∏
        r'["\']([^"\']*plans?[^"\']*\.pdf)["\']',  # PDF –ø–ª–∞–Ω—ã
        r'["\']([^"\']*curriculum[^"\']*\.pdf)["\']',  # Curriculum PDF
        r'["\']([^"\']*study[^"\']*plan[^"\']*\.pdf)["\']',  # Study plan PDF
        r'baseURL:\s*["\']([^"\']+)["\']',  # Base URL
        r'["\']([^"\']*file_storage[^"\']*)["\']',  # File storage paths
        r'/api/v\d+/[^"\']*',  # API –≤–µ—Ä—Å–∏–∏
        r'https://abitlk\.itmo\.ru/api/[^"\']*',  # –ü—Ä—è–º—ã–µ API —Å—Å—ã–ª–∫–∏
    ]

    for pattern in patterns:
        matches = re.findall(pattern, js_content, re.IGNORECASE)
        endpoints.extend(matches)

    return list(set(endpoints))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã

def find_study_plan_via_api(base_url: str, program_slug: str = "ai") -> Optional[str]:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ —É—á–µ–±–Ω–æ–≥–æ –ø–ª–∞–Ω–∞ —á–µ—Ä–µ–∑ API endpoints
    –†–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±—ã–º–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∞–º–∏ –ò–¢–ú–û (ai, ai_product, highload_systems, etc.)
    """
    import requests

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'application/json',
        'Content-Type': 'application/json',
        'Accept-Language': 'ru'
    }

    # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π API endpoint –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ–≥—Ä–∞–º–º –º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä—ã
    api_endpoint = f"https://abitlk.itmo.ru/api/v1/programs/master/{program_slug}"

    try:
        print(f"üåê –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å API: {api_endpoint}")
        response = requests.get(api_endpoint, headers=headers, timeout=15)

        if response.status_code == 200:
            data = response.json()
            print(f"‚úÖ –£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç API: {len(str(data))} —Å–∏–º–≤–æ–ª–æ–≤")

            # –ò—â–µ–º academic_plan –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
            if data.get('ok') and 'result' in data and 'academic_plan' in data['result']:
                pdf_url = data['result']['academic_plan']
                print(f"üìã –ù–∞–π–¥–µ–Ω academic_plan: {pdf_url}")
                return pdf_url

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ PDF —Å—Å—ã–ª–æ–∫ –≤ JSON –¥–ª—è –ª—é–±—ã—Ö –ø—Ä–æ–≥—Ä–∞–º–º
            json_str = json.dumps(data, ensure_ascii=False)
            pdf_matches = re.findall(r'https?://[^\s"\']*\.pdf[^\s"\']*', json_str)
            if pdf_matches:
                # –§–∏–ª—å—Ç—Ä—É–µ–º PDF, –∏—Å–∫–ª—é—á–∞—è —ç–∫–∑–∞–º–µ–Ω—ã
                for pdf_url in pdf_matches:
                    if 'plan' in pdf_url.lower() or 'curriculum' in pdf_url.lower():
                        print(f"üìÑ –ù–∞–π–¥–µ–Ω PDF —É—á–µ–±–Ω–æ–≥–æ –ø–ª–∞–Ω–∞: {pdf_url}")
                        return pdf_url

                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π (–Ω–æ –Ω–µ —ç–∫–∑–∞–º–µ–Ω—ã)
                for pdf_url in pdf_matches:
                    if 'exam' not in pdf_url.lower():
                        print(f"üìÑ –ù–∞–π–¥–µ–Ω PDF (–æ–±—â–∏–π): {pdf_url}")
                        return pdf_url

        else:
            print(f"‚ùå API –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É: {response.status_code}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ API: {e}")

    return None

def find_study_plan_url_advanced(html_content: str, base_url: str) -> Optional[str]:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫ —Å—Å—ã–ª–∫–∏ –Ω–∞ —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤
    """
    from bs4 import BeautifulSoup
    from urllib.parse import urljoin
    import requests

    soup = BeautifulSoup(html_content, "html.parser")

    # –ò–∑–≤–ª–µ–∫–∞–µ–º slug –ø—Ä–æ–≥—Ä–∞–º–º—ã –∏–∑ URL
    program_slug = "ai"  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    if "/program/master/" in base_url:
        program_slug = base_url.split("/program/master/")[-1].split("/")[0]

    print(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω slug –ø—Ä–æ–≥—Ä–∞–º–º—ã: {program_slug}")

    # –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API endpoints
    print("=== –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API ===")
    api_result = find_study_plan_via_api(base_url, program_slug)
    if api_result:
        return api_result

    # –ú–µ—Ç–æ–¥ 2: –ê–Ω–∞–ª–∏–∑ JavaScript —Ñ–∞–π–ª–æ–≤
    print("=== –ú–µ—Ç–æ–¥ 2: –ê–Ω–∞–ª–∏–∑ JavaScript —Ñ–∞–π–ª–æ–≤ ===")
    js_urls = extract_js_urls_and_analyze(html_content, base_url)
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(js_urls)} JavaScript —Ñ–∞–π–ª–æ–≤")

    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}

    for js_url in js_urls[:10]:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 10 JS —Ñ–∞–π–ª–æ–≤
        try:
            print(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º JS: {js_url}")
            js_response = requests.get(js_url, headers=headers, timeout=15)
            if js_response.status_code == 200:
                endpoints = analyze_js_for_api_endpoints(js_response.text)
                print(f"–ù–∞–π–¥–µ–Ω–æ {len(endpoints)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö endpoints")

                for endpoint in endpoints:
                    if endpoint.endswith('.pdf') and any(keyword in endpoint.lower() for keyword in ["plan", "curriculum", "study"]):
                        if endpoint.startswith('http'):
                            return endpoint
                        else:
                            return urljoin(base_url, endpoint)

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ JS {js_url}: {e}")
            continue

    # –ú–µ—Ç–æ–¥ 3: –ü–æ–∏—Å–∫ –ø–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –ò–¢–ú–û (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–¥)
    print("=== –ú–µ—Ç–æ–¥ 3: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã ===")
    program_codes = [program_slug]

    # –ò–∑ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    title_elem = soup.find("h1")
    if title_elem:
        title = title_elem.get_text().lower()
        if "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç" in title and "–ø—Ä–æ–¥—É–∫—Ç" not in title:
            program_codes.append("ai")
        elif "–ø—Ä–æ–¥—É–∫—Ç" in title and "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π" in title:
            program_codes.append("ai_product")

    # –ü—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ URL-—ã –¥–ª—è —É—á–µ–±–Ω—ã—Ö –ø–ª–∞–Ω–æ–≤ –ò–¢–ú–û
    base_plan_url = "https://abit.itmo.ru/file_storage/file/plans/master/"
    for code in program_codes:
        potential_urls = [
            f"{base_plan_url}{code}.pdf",
            f"{base_plan_url}{code}_plan.pdf",
            f"{base_plan_url}plan_{code}.pdf"
        ]

        for url in potential_urls:
            try:
                response = requests.head(url, headers=headers, timeout=10)
                if response.status_code == 200:
                    print(f"–ù–∞–π–¥–µ–Ω —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω –ø–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É URL: {url}")
                    return url
            except:
                continue

    return None

def parse_from_urls(urls: List[str]) -> Dict[str, Any]:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–æ–≥—Ä–∞–º–º —Å –ª—é–±—ã—Ö URL-–æ–≤
    """
    try:
        import requests
        from bs4 import BeautifulSoup
        from urllib.parse import urljoin

        def fetch_webpage(url: str) -> Optional[str]:
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                response = requests.get(url, headers=headers, timeout=30)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e}")
                return None

        def parse_program_page(html_content: str, base_url: str = "") -> Dict[str, Any]:
            soup = BeautifulSoup(html_content, "html.parser")
            result = {}

            # –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã
            title = soup.find("h1")
            result["title"] = title.text.strip() if title else ""

            # –û–ø–∏—Å–∞–Ω–∏–µ
            result["description"] = ""
            desc_selectors = [
                'span[class*="AboutProgram_aboutProgram__lead"]',
                'span[class*="AboutProgram_aboutProgram__description"]'
            ]

            for selector in desc_selectors:
                desc_block = soup.select_one(selector)
                if desc_block:
                    result["description"] = desc_block.get_text(separator=" ", strip=True)
                    break

            # FAQ
            result["faq"] = []
            for item in soup.select(".Accordion_accordion__item__A6W5t"):
                question_elem = item.select_one("h5")
                answer_elem = item.select_one("div[class*='info']")

                if question_elem and answer_elem:
                    result["faq"].append({
                        "question": question_elem.get_text(strip=True),
                        "answer": answer_elem.get_text(" ", strip=True)
                    })

            # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–∫–∏ –Ω–∞ —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω - —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
            result["study_plan_url"] = ""

            # –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ –ø—Ä—è–º—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ PDF
            for link in soup.find_all("a", href=True):
                href = link.get("href")
                link_text = link.get_text().lower()

                if ("–ø–ª–∞–Ω" in link_text or "curriculum" in link_text) and href.endswith(".pdf"):
                    if href.startswith("http"):
                        result["study_plan_url"] = href
                    elif base_url:
                        result["study_plan_url"] = urljoin(base_url, href)
                    break

            # –ú–µ—Ç–æ–¥ 2: –ü–æ–∏—Å–∫ –≤ JavaScript –∫–æ–¥–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if not result["study_plan_url"]:
                script_tags = soup.find_all("script")
                for script in script_tags:
                    if script.string:
                        script_content = script.string
                        # –ò—â–µ–º PDF —Å—Å—ã–ª–∫–∏ –≤ JavaScript
                        pdf_matches = re.findall(r'["\']([^"\']*\.pdf)["\']', script_content)
                        for pdf_url in pdf_matches:
                            if "plan" in pdf_url.lower() or "curriculum" in pdf_url.lower():
                                if pdf_url.startswith("http"):
                                    result["study_plan_url"] = pdf_url
                                elif base_url:
                                    result["study_plan_url"] = urljoin(base_url, pdf_url)
                                break
                        if result["study_plan_url"]:
                            break

            # –ú–µ—Ç–æ–¥ 3: –ü–æ–∏—Å–∫ –∫–Ω–æ–ø–∫–∏ "–°–∫–∞—á–∞—Ç—å —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω" –∏ –ø–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ —Å–≤—è–∑–∞–Ω–Ω—É—é —Å—Å—ã–ª–∫—É
            if not result["study_plan_url"]:
                study_plan_buttons = soup.find_all("button", string=re.compile(r"—Å–∫–∞—á–∞—Ç—å.*–ø–ª–∞–Ω", re.IGNORECASE))
                for button in study_plan_buttons:
                    # –ò—â–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π —ç–ª–µ–º–µ–Ω—Ç —Å —Å—Å—ã–ª–∫–æ–π
                    parent = button.parent
                    while parent and parent.name != "body":
                        if parent.name == "a" and parent.get("href"):
                            href = parent.get("href")
                            if href.endswith(".pdf"):
                                if href.startswith("http"):
                                    result["study_plan_url"] = href
                                elif base_url:
                                    result["study_plan_url"] = urljoin(base_url, href)
                                break
                        parent = parent.parent
                    if result["study_plan_url"]:
                        break

            # –ú–µ—Ç–æ–¥ 4: –ü–æ–∏—Å–∫ –ø–æ data-–∞—Ç—Ä–∏–±—É—Ç–∞–º
            if not result["study_plan_url"]:
                elements_with_data = soup.find_all(attrs={"data-url": True})
                for element in elements_with_data:
                    data_url = element.get("data-url")
                    if data_url and data_url.endswith(".pdf"):
                        if "plan" in data_url.lower() or "curriculum" in data_url.lower():
                            if data_url.startswith("http"):
                                result["study_plan_url"] = data_url
                            elif base_url:
                                result["study_plan_url"] = urljoin(base_url, data_url)
                            break

            # –ú–µ—Ç–æ–¥ 5: –ü–æ–∏—Å–∫ –≤—Å–µ—Ö PDF —Å—Å—ã–ª–æ–∫ –≤ HTML –∏ –≤—ã–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–π
            if not result["study_plan_url"]:
                all_pdf_links = []
                # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ PDF
                for element in soup.find_all(href=re.compile(r'.*\.pdf$')):
                    href = element.get("href")
                    if href:
                        if href.startswith("http"):
                            all_pdf_links.append(href)
                        elif base_url:
                            all_pdf_links.append(urljoin(base_url, href))

                # –¢–∞–∫–∂–µ –∏—â–µ–º PDF —Å—Å—ã–ª–∫–∏ –≤ —Ç–µ–∫—Å—Ç–µ
                html_text = str(soup)
                pdf_matches = re.findall(r'https?://[^\s<>"]+\.pdf', html_text)
                all_pdf_links.extend(pdf_matches)

                # –í—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â—É—é —Å—Å—ã–ª–∫—É
                for pdf_link in all_pdf_links:
                    if any(keyword in pdf_link.lower() for keyword in ["plan", "curriculum", "—É—á–µ–±–Ω—ã–π", "–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è"]):
                        result["study_plan_url"] = pdf_link
                        break

                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º, –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é PDF —Å—Å—ã–ª–∫—É
                if not result["study_plan_url"] and all_pdf_links:
                    result["study_plan_url"] = all_pdf_links[0]

            return result

        programs = {}

        for url in urls:
            print(f"\n=== –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–≥—Ä–∞–º–º—ã —Å URL: {url} ===")
            html_content = fetch_webpage(url)

            if html_content:
                try:
                    program_data = parse_program_page(html_content, url)
                    program_name = program_data.get("title", f"–ü—Ä–æ–≥—Ä–∞–º–º–∞_{len(programs)+1}")

                    programs[program_name] = program_data
                    programs[program_name]["source_url"] = url

                    print(f"–£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∞: {program_name}")

                    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º PDF —Å —ç–∫–∑–∞–º–µ–Ω–∞–º–∏ –∏–∑ HTML, —Å—Ä–∞–∑—É –∏—â–µ–º —á–µ—Ä–µ–∑ API
                    study_plan_url = program_data.get("study_plan_url")
                    if study_plan_url and "exams" in study_plan_url:
                        print(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω PDF —Å —ç–∫–∑–∞–º–µ–Ω–∞–º–∏, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º: {study_plan_url}")
                        study_plan_url = None

                    # –ò—â–µ–º –Ω–∞—Å—Ç–æ—è—â–∏–π —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω —á–µ—Ä–µ–∑ API
                    if not study_plan_url:
                        print("üîç –ò—â–µ–º —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω —á–µ—Ä–µ–∑ API...")
                        study_plan_url = find_study_plan_url_advanced(html_content, url)
                        if study_plan_url:
                            programs[program_name]["study_plan_url"] = study_plan_url

                    # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—Å—ã–ª–∫–∞ –Ω–∞ —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω, —Å–∫–∞—á–∏–≤–∞–µ–º –∏ –ø–∞—Ä—Å–∏–º PDF
                    if study_plan_url:
                        print(f"üì• –ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –Ω–∞ —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω: {study_plan_url}")
                        pdf_data = download_and_parse_pdf(study_plan_url)
                        if pdf_data and pdf_data["curriculum"]["all_courses"]:
                            programs[program_name]["curriculum"] = pdf_data["curriculum"]
                            programs[program_name]["pdf_info"] = {
                                "text_length": pdf_data["pdf_text_length"],
                                "pages": pdf_data["pdf_pages"]
                            }
                            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω: {len(pdf_data['curriculum']['all_courses'])} –¥–∏—Å—Ü–∏–ø–ª–∏–Ω")
                        else:
                            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å PDF")
                    else:
                        print("‚ùå –°—Å—ã–ª–∫–∞ –Ω–∞ —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}")
                    programs[f"–û—à–∏–±–∫–∞_{len(programs)+1}"] = {
                        "title": f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞",
                        "error": str(e),
                        "source_url": url
                    }
            else:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å {url}")

        return programs

    except ImportError:
        print("–î–ª—è —Ä–∞–±–æ—Ç—ã —Å URL-–∞–º–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install requests beautifulsoup4")
        return {}

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
        print("  python final_parser.py <url1> [url2] [url3] ...")
        return

    # –†–µ–∂–∏–º –ø–∞—Ä—Å–∏–Ω–≥–∞ URL-–æ–≤
    urls = sys.argv[1:]
    print(f"–ü–∞—Ä—Å–∏–Ω–≥ {len(urls)} URL-–æ–≤...")

    programs = parse_from_urls(urls)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    output_file = "parsed_programs.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(programs, f, indent=2, ensure_ascii=False)

    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")

    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    for program_name, data in programs.items():
        if "error" in data:
            print(f"‚ùå {program_name}: {data['error']}")
        else:
            faq_count = len(data.get('faq', []))
            print(f"‚úÖ {program_name}: {faq_count} FAQ")

if __name__ == "__main__":
    main()

